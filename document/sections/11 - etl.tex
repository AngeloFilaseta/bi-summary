\section{Progettazione dell'ETL}
Ora abbiamo gli schemi logici del Data Mart e dobbiamo creare la struttura fisica, la prima cosa da fare è progettare l'ETL. Questa si tratta della fase più lunga e complessa. In questo caso infatti bisogna scrivere del software. Ci sono processi che sono identici indipendentemente dal progetto, ma ci sono anche parti estremamente variabili.

\noindent Una volta che abbiamo l'ODS, l'ETL va progettato in due fasi:
\begin{itemize}
	\item \textbf{Dalle sorgenti operazionali all'ODS};
	\item \textbf{Dall'ODS al Data Mart};
\end{itemize}

\subsection{Alimentazione dell'ODS}
I dati vengono estratti dalle sorgenti e finiscono dentro la Staging Area, un'area di lavoro in cui si puliscono e trasformano i dati. La Staging Area è completamente invisibile all'utente. Una volta effettuata questa operazione i dati vengono caricati sull'ODS.

\centeredImage{img/odsetl.png}{Alimentazione dell'ETL}{0.5}

\subsubsection{Estrazione dei Dati}

\noindent L'estrazione può essere di due tipi:
\begin{itemize}
	\item \textbf{Statica}: Si estrae tutto ciò che c'è nei database sorgenti;
	\item \textbf{Incrementale}: Si estraggono solo i dati che sono cambiati dalla sorgente rispetto all'ultima volta che l'ETL è stato eseguito. Si distinguono due tipi di estrazione incrementale:
	\begin{itemize}
		\item \textbf{Immediata}: viene lanciata non appena si verifica un cambiamento nella base di dati di partenza;
		\item \textbf{Ritardata}: viene lanciata in un certo periodo temporale dopo la modifica.
	\end{itemize}
\end{itemize}

\noindent Le sorgenti operazionali contengono dati di natura diversa:
\begin{itemize}
	\item \textbf{Transitoria}: Viene mantenuta solo un'immagine corrente del sistema, i  dati vecchi vengono sovrascritti;
	\item \textbf{Storicizzata}: Si vuole tenere traccia di tutte le modifiche che sono avvenute nel tempo. Per farlo di solito è sufficiente  aggiungere una colonna data di tipo timestamp.
	\item \textbf{Semi-storicizzata}: Viene tenuta traccia solo di un numero fisso di stati precedenti (ad esempio mantengo solo le ultime 3 password utilizzate da un utente). Questo in pratica può essere realizzato aggiungendo un certo numero di campi anziché uno solo (es: invece di \textit{hash\_password} il record contiene \textit{hash\_password} e \textit{hash\_password\_precedente}).
\end{itemize}
\noindent Come riferimento abbiamo questa architettura:\newline
Una sorgente gestita da un DBMS. Sopra ci sarà una logica aplicativa che si appoggia su una API comune. In azzurro ho una tabella della Staging Area in cui voglio scaricare tutte le modifiche. Quando si estrae incrementalmente, lì finiscono i record cambiati nell'ultimo intervallo dell'ETL in modo che possano essere elaborati.
\centeredImage{img/etlextractionarch.png}{Architettura di riferimento per l'estrazione dati}{0.4}

\paragraph{Estrazione assistita}
Si tratta di una tecnica di estrazione immediata. Si modifica la logica applicativa. Ogni volta che c'è una modifica sui record queste vengono quindi rilevate e sono proprio queste funzioni che attraverso l'API popolano la tabella dei dati storicizzati. In teoria è un'ottima soluzione, ma in pratica questo comporta una massiccia dose di lavoro per modificare la logica applicativa. Una soluzione di questo tipo è quindi impraticabile.

\centeredImage{img/assist.png}{Estrazione Assistita}{0.4}

\paragraph{Estrazione basata su log}
Si tratta di una tecnica di estrazione ritardata. Per ogni operazione che viene effettuata il DBMS crea dei file di log per tenerne traccia. A questo punto è sufficiente, quando lanciamo l'ETL, recuperare il log e interpretando il log si riesce a costruire una catena di cambiamenti intercorsi. In teoria è facile, in pratica anche questo metodo in pratica è un incubo. Il DBMS nei log scrive moltisisme informazioni. Riuscire ad estrarre le parti utili è un'operazione estremamente complessa. Capita però che il vendor del DBMS fornisce anche un modulo per interpretare i log.
\centeredImage{img/logbase.png}{Estrazione basata su log}{0.4}

\paragraph{Estrazione basata su trigger}
Si tratta di una tecnica di estrazione immediata. I trigger sono procedure supportate da qualsiasi DBMS. Queste procedure vengono agganciate a particolari istruzioni su particolari tabelle o colonne di tabelle. Quando accade un evento (es: inserimento di un nuovo record), se è vera una certa condizione, allora avviene una certa azione (una vera e propria procedura che è possibile scrivere).
\noindent Quindi è possibile abbinare ad ogni tabella un trigger. Quando una riga di una tabella viene modificata, la riga viene riscritta anche dentro la staging area. Il problema in questo caso ricade sia sul fatto che bisogna scrivere tanti trigger, a volte anche diversi tra loro. Inoltre c'è un problema di prestazioni. Il DBMS per ogni operazione che compie deve capire se ci sono trigger la lanciare e quali. Molta potenza viene quindi utilizzata per fare monitoraggio completo della situazione. Avere ritardi sui DB operazionali non è il massimo. Questa soluzione non è perfetta.
\centeredImage{img/trigger.png}{Estrazione basata su trigger}{0.4}

\paragraph{Estrazione basata su marche temporali}
Si tratta di una tecnica di estrazione ritardata. Se nel DB operazionale per ogni record ho una colonna con un timestamp del tipo \textit{last\_time\_modified} allora è possibile fare un'estrazione con clausola WHERE specificando che il dato deve essere maggior della data di ultimo ETL. Il problema di questo approccio è che se non è presente questa colonna, è possibile aggiungerle, ma ci sono anche in questi casi problemi di costi. Fortunatamente nei DBMS odierni questa colonna esiste sempre.
\begin{warn}
	Attenzione solo ad un dettaglio. Utilizzando questa procedura è possibile perdersi dei dettagli. Ipotizziamo che l'ETL venga lanciato ogni 7 giorni. Se un record viene modificato 5 volte entro quel lasso di tempo solo l'ultima modifica viene recepita dall'ETL. Questo non sempre è un problema, dipende fortemente dal contesto. Sicuramente si tratta di un dettaglio non trascurabile.
\end{warn}
\centeredImage{img/temporal.png}{Estrazione basata su marche temporali}{0.5}

\subsection{Risultati dell'estrazione}
Indipendentemente dalla tecnica utilizzata, il risultato della fase di estrazione consiste nell'insieme di record della sorgente che sono stati modificati, aggiunti o cancellati. A ogni record viene associato il tipo di operazione che ha generato la variazione (INSERT, UPDATE, DELETE).
\centeredImage{img/changes.png}{Tabella dei cambiamenti}{0.6}

\subsection{Caricamento dei dati}
La modalità di caricamento dei dati nella staging area dipende dalla tecnica di estrazione e dal livello di storicizzazione dell'ODS:
\begin{itemize}
	\item \textbf{Estrazione Statica} $\xrightarrow{}$ \textbf{Riscrittura completa dell'ODS};
	\item \textbf{Estrazione incrementale} $\xrightarrow{}$
	\begin{itemize}
		\item Se \textbf{l'ODS non è storicizzato}: memorizzo solo il tipo di operazione che ha determinato la variazione;
		\item Se \textbf{l'ODS è storicizzato}: memorizzo anche una coppia di marche temporali che indicano l'intervallo di validità della tupla;
	\end{itemize}
\end{itemize}

\subsubsection{Trasformazione e pulizia}
I dati vengono quindi caricati nella Staging Area. In questo momento i dati devono essere ripuliti e trasformati. Molti errori sono dovuti a:
\begin{itemize}
	\item \textbf{Errori di battitura};
	\item \textbf{Differenze di formati dei dati};
	\item \textbf{Inconsistenze tra valori e descrizioni};
	\item \textbf{Inconsistenze tra valori di campi correlati}: Ad esempio il codice fiscale non combacia con il luogo di nascita;
\end{itemize}
\noindent Ogni problema richiede una tecnica specifica per la soluzione. Il corso di Sistemi Informativi ha già approfondito questi aspetti, quindi qua vengono solo riassunti:

\begin{itemize}
	\item \textbf{Tecniche basate su dizionari}: si utilizzano tabelle di look-up per identificare ed eliminare sinonimi e abbreviazioni. Sono utilizzabili solo quando il dominio dell'attributo è conosciuto e limitato. In compenso sono utili per errori di battitura e discrepanze di formato;
	\item \textbf{Tecniche ad hoc}: ogni dominio applicativo ha regole proprie, troppo specifiche per essere verificate tramite strumenti standard. Ad esempio per equazioni (es: \textit{profitto = guadagno - spese});
	\item \textbf{Tecniche di fusione approssimata}: permettono di identificare record corrispondenti in assenza di identificatori comuni grazie ad esempio a Join approssimati.
\end{itemize}

\subsection{Alimentazione della DT}
Nell'ODS avremo delle tabelle con dei dati che sono stati modificati. Per costruire una tupla che finirà in DT probabilmente dovrò fare dei Join, essendo i dati nelle dimension table spesso denormalizzati. Il problema che rimane è che le chiave dell'ODS e le chiave del Data Mart non combaciano, essendo due basi di dati completamente diverse. A questo scopo si mantiene nella staging area una tabella contenente le corrispondenze tra le chiavi. Questa si tratta di una gestione generale. Ogni implementazione contiene più dettagli.

\centeredImage{img/dtalim.png}{Processo di alimentazione di una DT}{0.6}

\subsection{Alimentazione della FT}
Per la FT l'operazione è molto simile. Vengono create le tuple della Fact Table attraverso le solite operazioni di Join e di Group By. Anche in questo caso c'è il problema delle chiavi, risolvibile con lo stesso mapping già descritto.

\centeredImage{img/ftalim.png}{Processo di alimentazione di una FT}{0.6}

\subsection{Alimentazione delle viste}
È importante mantenere anche le viste sincronizzate se si è scelto di materializzarne. Se ne abbiamo più di una ricordiamoci che spesso è possibile sfruttarle per crearne altre semplicemente aggregando. In questo modo diminuiscono i tempi di aggiornamento.
